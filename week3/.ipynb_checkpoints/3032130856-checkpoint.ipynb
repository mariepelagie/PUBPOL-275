{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Public Policy 275] Spatial Data and Analysis, Fall 2019\n",
    "\n",
    "# Lab 3: Point Processes\n",
    "\n",
    "* University of California, Berkeley\n",
    "* Instructor: Solomon Hsiang\n",
    "* TA: Luna Yue Huang\n",
    "\n",
    "**About This Lab**\n",
    "* Due Date: Sep 18, 2019\n",
    "* You may talk with other students about the lab, but each student is responsible for doing all exercises in the lab themselves and turning in their own write up. We will be checking code for evidence of copying and pasting.\n",
    "* When you are done with the lab, please restart the kernel and run all the codes one more time (press the \">>\" button), so that the grader will see a clean notebook. Don't forget to save! Please submit this lab to bCourses as `YOURCAL1ID.ipynb`. This will facilitate anonymous grading.\n",
    "* The class just recently transitioned from Matlab to Python. This is a newly developed assignment, so if you think there is an error or something is unclear, let us know right away. That will be extremely helpful to us and your fellow students.\n",
    "* This lab requires that you download the following files and place them in the same directory as this jupyter notebook:\n",
    "    * `Lab3_crime.csv`\n",
    "    * `Lab3_cholera.p`\n",
    "    * `Lab3_poisson_test_data.p`\n",
    "* These labs have been developed over the years by Solomon Hsiang, and past/current GSIs including Ian Bolliger, Tamma Carleton, Shubham Goel, Felipe Gonz√°lez, Luna Yue Huang, Jonathan Kadish, and Jonathan Proctor (in alphabetical order)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "By the end of this lab, you will be able to\n",
    "1. Get to know `pandas`\n",
    "2. Understand functions, classes, attributes and methods in Python\n",
    "3. Run simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT MODULES HERE\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyzing Crimes by Attributes\n",
    "Use the dataset `Lab3_crime.csv`, it contains the location of 200 crimes as well as information on whether the crime was violent (the variable `violent`=1 if violent, 0 otherwise) and the month in which the crime was committed (the variable `month`). Here we want to see if there are any patterns in crime based on their attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.**\n",
    "\n",
    "So far, we have worked with vectors and arrays where information across different vectors might be related (e.g., the first element in a vector lat and a vector lon describe the same position), but we had to keep track ourselves of these various relationships. However, Python will allow us to link vectors or arrays of strings the way we sometimes see datasets structured in the software packages Excel, Stata or R, where individual rows represent observations and columns are different variables. This data format is more restrictive than what we have done so far, but it is often convenient for working with data sets that come in this format since it may help us keep variables organized. To work with data in this format, we can utilize the data structure table, or `DataFrame`, as Python calls it. This data format is also useful because it allows us to read in data from these familiar software packages (keeping strings and numeric data grouped properly) so we can manipulate it in Python. From now on, we will be seeing a lot of the `pandas` module, which is what handles data frames in Python.\n",
    "\n",
    "To use pandas, you would want to first `import` the pandas module (you don't need to install it because it comes with the Anaconda distribution). The convention is to alias it with `pd`, so you would do `import pandas as pd`. Then you would want to read a csv file (using `pd.read_csv()`) and assign it to `df`, which is what people usually call their data frames. Type the variable name `df` to print the data frame in jupyter notebook, and you will see a nicely formatted table.\n",
    "\n",
    "**Let's make a map of all crime events. Choose a symbol that we haven't used before.**\n",
    "\n",
    "(Hint: `pandas.DataFrame` is very much like a dictionary. So you can access columns with `[]`, e.g., `df['violent']` gives the `violent` column in the data frame. A one-column object is called a `pandas.Series`, e.g., `df['violent']` is a pandas series. matplotlib works well with pandas, so you can directly pass a pandas series (as we did with numpy arrays) to `plt.plot()` in order to plot maps (no need to convert it to numpy objects!).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: READ THE DATASET, PLOT THEIR GEOGRAPHICAL LOCATIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.**\n",
    "\n",
    "Now we want to see if the spatial pattern in violent and non-violent crime is different by plotting them as two different kinds of symbols. We could do this by writing a loop like we did for houses in Lab 2, but this is slow. Python has a simple notation that allows us to more easily isolate observations. This is usually called boolean (a boolean variable takes two values, True or False) indexing. Basically, we want to create a vector (more precisely, a `numpy.ndarray` or a `pandas.Series`) that has the same number of rows as our data frame, and takes on either `True` or `False`, and we will extract all the rows in the data frame with a corresponding `True` value in the vector.\n",
    "\n",
    "Type `df['violent'] == 1` first and watch what happens.\n",
    "\n",
    "The double-equal sign `==` tells Python to check for equality, rather than assigning values (`a = 1` means \"assign a equal to one,\" while `a == 1` means \"check if a is equal to 1\"). `df['violent'] == 1` lets Python return a new vector that is True for each row where the statement is True (the incident is violent), and False where it is False (the incident is non-violent). We can use boolean indexing to identify the crime observations based on the attributes of crimes (whether it is violent, in which month it occurred, etc.).\n",
    "\n",
    "Boolean indexing in pandas can be achieved through `df.loc[row_index, col_index]`. You have to specify two indices, one for row and one for column. For the rows, put the boolean vector in the square brackets and you will get the subset of rows (where the boolean vector says `True`). For the columns, put in `:` to mean that you are selecting all the columns, or put in a string (column name) to select one column.\n",
    "\n",
    "**Now, plot the spatial distribution of violent and non-violent crimes on the same map, using different markers for the two. Be sure to label the plot properly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Quick Aside] Coding Style: Maximum Line Length\n",
    "It is never too early to talk about coding style. PEP-8, the official Python style guide, says that each line of code should not exceed 79 characters. So when you write codes, it is important to wrap long lines appropriately.\n",
    "\n",
    "Why is that? First, most code editors (this applies to Jupyter Notebook, too) do not wrap lines by default. For a language like Python, where line breaks and leading white spaces have semantic meaning, wrapping lines would make codes confusing to the readers. Second, super long lines are almost impossible to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'wouldn\\'t you hate to read this line of code that is so incredibly very very very very very very very very very very very very long?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right way to wrap long lines is to use Python's implied line continuation inside parentheses, brackets and braces. If necessary, you can add an extra pair of parentheses around an expression. Here are some good practices.\n",
    "\n",
    "```python\n",
    "# implied line continuation inside parentheses\n",
    "function(param1=argument1,\n",
    "         param2=argument2,\n",
    "         param3=argument3,\n",
    "         param4=argument4,\n",
    "         param5=argument5,\n",
    "         param6=argument6)\n",
    "# sometimes people like to do this\n",
    "# enter a line break right after the parenthese/brackets/braces\n",
    "# and indent by 4 spaces\n",
    "d = {  # this is creating a dictionary\n",
    "    'key1': 'value1',\n",
    "    'key2': 'value2',\n",
    "    'key3': 'value3',\n",
    "    'key4': 'value4',  # leaving a comma here is fine\n",
    "}\n",
    "# for long strings, and with .format()\n",
    "print('{} a long long string '\n",
    "      'that is broken into several parts'\n",
    "      .format('this is'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLOT YOUR MAP HERE\n",
    "# USE DIFFERENT MARKERS FOR VIOLENT VS NON VIOLENT EVENTS\n",
    "# WRAP LONG LINES APPROPRIATELY\n",
    "# MAKE SURE THAT EACH LINE DOES NOT EXCEED 79 CHARACTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Indexing in pandas\n",
    "\n",
    "`df.loc[]` allows us to index **by label**. In pandas, all the rows and columns have labels. Usually, the row labels (a.k.a. indexes) are integers 0,1,2,... (they don't have to be though, they can be strings or other data types), and column names are descriptive strings. For our `df`, each row has a integer ranging from 0 to 199 identifying that observation, and we can select the three observations with the indexes 3, 4 and 5 and assign it to `df1` by doing the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[[3, 4, 5], :]\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the indexes (row labels) are preserved. The first row has an index of 3 instead of 0. Now if you try to do `df1.loc[0, :]`, there will be a `KeyError` because `df1` doesn't have any observations associated with the index 0.\n",
    "\n",
    "We can, however, also index **by position** by doing `df.iloc[]`. The following will give you the first two observations in `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.iloc[0:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen, `loc[]` and `iloc[]` take in a variety of types of inputs to index data frames. They take in slice objects (e.g., `0:2`), lists, boolean arrays, etc. The [official documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) goes into more detail on accepted input types.\n",
    "\n",
    "*Indexing and selecting data in pandas can be rather tricky. It can take some time for beginners to get used to it. Be patient, and always visually check your data frame to make sure that you are doing what you think you are doing!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.**\n",
    "\n",
    "Now examine how the distribution of all crimes evolves month by month by plotting each month separately. To make comparisons across months easier, let's put them all on the same figure as 12 different panels (e.g. January will be a small map next to a separate small map for February). To do this, create a new figure and use the command `plt.subplots()`. subplot will break the figure up into smaller \"subplots\" while also telling Python which of these subplots you want to be drawing on with your current commands. Type `fig, axes = plt.subplots(nrows=3, ncols=4, sharex=True, sharey=True)` to make an array of 12 subplots (3 down and 4 across), with the same x and y axes.\n",
    "\n",
    "This way of creating two things at the same time is very Pythonic and is usually called multiple assignment. It helps keep your codes succint and readable. `plt.subplots()` returns two objects, and we name the first one `fig`, which is basically the large plot, and the second one `axes`, which is a collection of subplots. Each subplot can be accessed through `[]`, e.g., `axes[0, 0]` is the first subplot.\n",
    "\n",
    "We can then specify what we want to do with each subplot. For example, we can do `axes[0, 0].plot(df['crime_x'], df['crime_y'], '+')` to plot onto the first subplot.\n",
    "\n",
    "**Write a short for loop that goes through the months and plots the locations of all crimes in a month on a separate subplot for that month. Put January in the upper left subplot. Help your viewers understand which month they are looking at by labeling each subplot with a title that says the month number for that subplot.** (Hint: this can be done via `axes[i, j].set_title()` and `''.format()`; if the figure looks awful because the labels are cut out / overlapping with each other, `fig.tight_layout()` usually provides a quick fix.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLOT YOUR MAP HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4.**\n",
    "\n",
    "How many crimes are there in March? How many are violent crimes? (Hint: there are multiple ways to do this, but you may find commands like `df.shape` helpful; you may need to use the logical operator `&` (and).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WRITE YOUR CODES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5.**\n",
    "\n",
    "Create a new attribute that describes the Euclidean distance from the location of each crime to the nearest violent crime (the value of this new attribute should be zero for all violent crimes). **Plot a histogram showing the distribution of these nearest-violent-crime distances using the command `plt.hist()`. Why does this distribution look the way it does?**\n",
    "\n",
    "Some tips:\n",
    "\n",
    "* Write a function that can be re-used so that your future self will thank you!\n",
    "* You will likely have to write a double loop, where you loop over the set of source points first, and then inside that loop, you loop over the set of destination points.\n",
    "* `pandas.DataFrame.values` converts the pandas data frame to a numpy array.\n",
    "* `numpy.ndarray.min(axis=1)` takes the minimum of an array along the columns (`axis=0` takes min along the rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(src_x, src_y, dst_x, dst_y):\n",
    "    \"\"\"Computes the distances from every source point to every destination point.\n",
    "    \n",
    "    Args:\n",
    "        src_x, src_y (numpy.ndarray): the x and y coordinates for source points.\n",
    "            Denote the length of each array as dim_src.\n",
    "        dst_x, dst_y (numpy.ndarray): the x and y coordinates for destination points.\n",
    "            Denote the length of each array as dim_dst.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: a dim_src by dim_dst numpy array containing the distances.\n",
    "    \"\"\"\n",
    "    # TODO: COMPLETE THIS FUNCTION\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: USING THE FUNCTION, ANSWER THE QUESTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: ANSWER THE QUESTION HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the `.` in Python\n",
    "### Namespace\n",
    "\n",
    "I think of name space as literally a \"name space\": we create a space for every name (variable names, class names, function names) that is defined in our current environment. Namespace is like a dictionary, a mapping between names and objects - it helps python understand what we mean when we type certain names. If we were to put all the names together in the global namespace (like R does) and make them easily accessible, we can potentially create chaos. What if popular keywords such as `filter`, are claimed by multiple packages (this is true in R)? How would R then know which function to call, and how would we know which function we are calling (we sometimes don't!). This can be very error prone.\n",
    "\n",
    "Python's approach to reducing overcrowding in these \"name spaces\" is to let each package have their own namespaces, and we can access their namespaces by using the dot notation, so `numpy.array()` means: the `array()` function in the `numpy` module. There can also be submodules (`numpy.random.random()` means the `random()` function in the `random` submodule in the `numpy` module). This leads to somewhat more verbose but much clearer and more predictable codes, which is particularly important for a multi-purpose language like Python (imagine how many packages would define their own `open()` function!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a really useful Jupyter Notebook magic command is %who\n",
    "# it will list all the names that are defined in the current environment\n",
    "%who\n",
    "# notice that the reserved keywords in core Python are not listed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the python command to do this is dir()\n",
    "dir()\n",
    "# you will notice that the outputs from here are messier\n",
    "# and many internal variables are exposed\n",
    "# because a lot is happening as Jupyter Notebook\n",
    "# hosts this very nice environment for you to work in\n",
    "# notice that the reserved keywords in core Python are not listed here\n",
    "# but you can see them with `dir(__builtins__)`\n",
    "# and they are still directly accessible to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't like df1 since it is a temporary variable\n",
    "# let's delete it\n",
    "del(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Oriented Programming\n",
    "What is object oriented programming? It is a paradigm where objects are at the center of the design of the language, not only representing the data, but in the overall structure of the program as well. It is often contrasted with functional programming, where functions are at the center.\n",
    "\n",
    "Python is an object oriented language, whereas some other languages, such as R and Matlab, are more functional (in the sense that most scripts that we see in R/Matlab are more functional). Both R and Matlab are growing in the OOP direction, though, and both support the OOP paradigm.\n",
    "\n",
    "In Python, there are many `class`es of `object`s. Each `class` has many `attribute`s and `method`s. An attribute is a \"field\" that stores data within the object, and a method is like a function, but defined solely on this particular class. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated (objects have a notion of `self`), which essentially means, altering the persistent \"state\" of the object.\n",
    "\n",
    "For example, suppose we have a data frame `df` (assuming that `df` is an `instance` of the `class` `pandas.DataFrame`), and we want to sort the rows based on its values in a certain column `order`. In Python, we would do this\n",
    "\n",
    "```python\n",
    "df.sort_values(by='order', inplace=True)\n",
    "```\n",
    "\n",
    "The dot between `df` and `sort_values()` denotes that the latter is a method of the former. So this is essentially saying that we take the object `df`, and we change the data within that object by calling the method `sort_values()`. Notice that we did not pass `df` to `sort_values()` in an argument! So how did the function know what it is supposed to perform the operation on? It implicitly remembers its \"parent\" (there is a strong convention in Python to call its \"parent\", the object, `self`).\n",
    "\n",
    "This would normally not be done in functional programming languages, such as R. In R (tidyverse), we would do this\n",
    "\n",
    "```R\n",
    "df = dplyr::arrange(df, order)\n",
    "```\n",
    "\n",
    "Notice that we have to tell the function `arrange()` that we want it to perform sorting on `df`, the first argument that we passed into the function. This is sometimes called a \"pure\" function: it does not alter the input, create any \"side effects\" (do anything other than generating the output), and all it does is returning the output.\n",
    "\n",
    "This may seem like a subtle difference at first, but it profoundly impacts the way that codes are structured in these programming languages. As we spend more and more time working in Python, and as we approach problems that are more and more complicated, this will become more obvious.\n",
    "\n",
    "All the Python modules are written in an object oriented manner. For example, `pandas` has a couple of important classes: `pandas.Series` and `pandas.DataFrame`, each of these have many, many methods that you can take advantage of to organize and modify your data frames. Note that all modifications are by default not in place (a new object is returned and the original object is not modified) unless the user specified `inplace=True`.\n",
    "\n",
    "### Reading Documentation\n",
    "\n",
    "Now let's try to read the pandas [official documentation](https://pandas.pydata.org/pandas-docs/stable/reference/index.html).\n",
    "\n",
    "Browse the webpage and click on [general functions](https://pandas.pydata.org/pandas-docs/stable/reference/general_functions.html). These are the \"pure\" functions that pandas defined. You would call them like this: `pd.concat()`.\n",
    "\n",
    "Click on `concat()` and you will see this:\n",
    "\n",
    "```python\n",
    "pandas.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=None, copy=True)\n",
    "```\n",
    "\n",
    "Lots of arguments! This is typical as pandas is a versatile package. The parameters without default values (`objs`) are the ones that you must pass arguments for (otherwise an error will be raised). If you want to override a default value, you can do that by passing that argument into the function, e.g., `pd.concat(df1, df2, axis=1)`. The documentation should explain what parameter does what.\n",
    "\n",
    "Now, browse the home webpage and click on [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html), which is perhaps the most important class in pandas. Click on [Constructor](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) - this explains how you could create a new `pandas.DataFrame` instance. Scroll down and you'll see that all the attributes and methods are listed. Attributes can be accessed through the dot notation (e.g., `df.columns` gives a list of column names). Methods can be called also using the dot notation (e.g, `df.describe()` describes the dataset). Click on any attribute or method to see the documentation of those, along with some examples.\n",
    "\n",
    "From now on, we will not be providing as many hints and explanations as before, as we will be assuming that you know how to find and read documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identifying Cholera Infection Hotspots\n",
    "The dataset `Lab3_cholera.p` contains the location of cholera cases during an outbreak (the sample was collected over one month). Your job is to identify cholera hotspots so that public health workers can figure out where the pathogens are coming from. To do this, you will have to estimate the intensity of cholera cases over the entire region (for values of x ranging from 1-10 and values of y ranging from -10 to +10).\n",
    "\n",
    "We will not use only the quadrat or kernel estimation techniques that we learned in class. Instead, we'll use a simple hybrid of the two. Imagine drawing a grid with lines at each integer value in x and at each integer value in y (this looks like a normal grid). We will go to each location $\\overrightarrow{\\underset{\\cdot}{c}}$ where these lines intersect and we will construct an estimate for the rate at which cholera cases arise (the infection rate) at that location $\\lambda(\\overrightarrow{\\underset{\\cdot}{c}})$ by using a kernel estimate.\n",
    "\n",
    "The kernel we'll use is sometimes called the \"uniform kernel\" because it assigns all observations uniform weight so long as they are within a certain cutoff distance $h$ (the cutoff distance is the bandwidth for this particular kernel). Imagine that we are standing at a point $\\overrightarrow{\\underset{\\cdot}{c}}$, a lattice point on our grid. We draw a circle of radius $h$ around our location. If any events occur within that circle, we count those events and assign them to our location $\\overrightarrow{\\underset{\\cdot}{c}}$ as an infection. However, because the infection rate $\\lambda(\\overrightarrow{\\underset{\\cdot}{c}})$ describes $\\frac{infections}{area}$, we must adjust this count for the area we are examining. In the case of this uniform kernel, the area we examine covers $\\pi \\times h^2$ with uniform weight, so we divide our infection count by this area to compute the rate of infection per unit area. We'll look for infection hotspots by plotting these infection rate estimates in a bubble plot, with larger bubbles depicting higher infection rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.**\n",
    "\n",
    "Begin by making sure that you understand the procedure. We will look at the set of points $\\overrightarrow{\\underset{\\cdot}{s}}=[x,y]$ where we use each pairwise combinations from the two sets\n",
    "$$x \\in \\{1,2,3,...,10\\}$$\n",
    "$$y \\in \\{-10,-9,-8,...,10\\}$$\n",
    "(Hint: you may want to draw this grid on scrap paper.) For each $\\overrightarrow{\\underset{\\cdot}{s}}$, we compute the Euclidean distance to the location of each cholera infection in the dataset. We then generate a new vector that is the weighted count of infections for $\\overrightarrow{\\underset{\\cdot}{s}}$, where the weights are determined by each infection site's distance from $\\overrightarrow{\\underset{\\cdot}{s}}$. The weights are determined by the kernel function, which in this case is the uniform kernel:\n",
    "\n",
    "$$\n",
    "\\kappa_{unif}\\left(\\frac{dist(\\overrightarrow{\\underset{\\cdot}{s}}, \\overrightarrow{\\underset{\\cdot}{s_{inf\\_site}}})}{h}\\right)=\\mathbf{1}\\left[ \\left(\\frac{dist(\\overrightarrow{\\underset{\\cdot}{s}}, \\overrightarrow{\\underset{\\cdot}{s_{inf\\_site}}})}{h}\\right) < 1 \\right] \\times \\frac{1}{\\pi}\n",
    "$$\n",
    "\n",
    "Where $dist(\\cdot)$ is the distance function, $\\overrightarrow{\\underset{\\cdot}{s}}_{inf\\_site}$ is the site of an infection event and $\\mathbf{1}[\\cdot]$ is the indicator function that is one if the statement in brackets is true. All this equation says is that an infection event is given a weight of one if its distance to the lattice point is less than $h$ (or equivalently that the distance divided by $h$ is less than one). The factor $\\frac{1}{\\pi}$ is just there to account for the circular structure of the area we are examining, as described above. (Hint: you may want to draw the shape of this kernel function on a sheet of scrap paper, where the horizontal axis is the distance from $\\overrightarrow{\\underset{\\cdot}{s}}$ and the vertical axis is $\\kappa$). Just like normal kernel estimation that we covered in class, we then estimate the infection rate at $\\overrightarrow{\\underset{\\cdot}{s}}$ by summing up these weighted values for all of the infection sites, which we index by $j$:\n",
    "$$\n",
    "\\hat{\\lambda}(\\overrightarrow{\\underset{\\cdot}{s}}) = \\sum_{j=1}^{N}\\left[ \\frac{1}{h^{2}}\\times \\kappa_{unif}\\left(\\frac{dist(\\overrightarrow{\\underset{\\cdot}{s}}, \\overrightarrow{\\underset{\\cdot}{s_{inf\\_site\\;j}}})}{h}\\right)\\right]\n",
    "$$\n",
    "which after subsitution and a little simplification looks less intimidating:\n",
    "$$\n",
    "\\hat{\\lambda}(\\overrightarrow{\\underset{\\cdot}{s}}) =\\frac{1}{\\pi h^{2}} \\times \\sum_{j=1}^{N} \\mathbf{1}\\left[ dist(\\overrightarrow{\\underset{\\cdot}{s}}, \\overrightarrow{\\underset{\\cdot}{s_{inf\\_site\\; j}}})< h \\right]\n",
    "$$\n",
    "The estimate for the infection rate at a location is just sum of several 1's and 0's, scaled by the area of a circle. The main challenge is to figure out which infection events to count as a 1 and which to count as a 0, which we do by measuring the distance to each event.\n",
    "\n",
    "Write a script that estimates $\\hat{\\lambda}(\\overrightarrow{\\underset{\\cdot}{s}})$ at each lattice point using the approach above. Some tips:\n",
    "\n",
    "* You should write your script so that the bandwidth variable `h` is defined once and only once, and you then just refer to this variable at subsequent points in the script (so you can easily change the bandwidth). In your first run, use `h=2`. This means we assign an infection to a lattice point if it's less than 2 units away.\n",
    "* In Python you can get the constant $\\pi$ by just typing `np.pi`. In your code, you can use `np.pi` as if it were a variable.\n",
    "* Use the function that you wrote for problem 1, and thank your past self for writing a generalizable function!\n",
    "* In order to use the function that you wrote, you need to change how you represent your lattice point coordinates. Think about what form of input the function takes in, and how to construct that input. You may need to use `numpy.meshgrid()` and `numpy.ndarray.flatten()`.\n",
    "* By default, logical and arithmetic operators in Python are executed on each element of a matrix/array, rather than on the entire matrix as a whole (matrix multiplication and exponentiation are also operations that Python performs, but you need to call other functions to do that). If you use these element-wise operations, Python returns a matrix that this the same size as the original. For example, if you type `a > 0`, python will check each element of `a` and give this element a value of `True` if it is positive and `False` otherwise. If `a` and `b` are vectors (`numpy.ndarray`) of the same size, then `a * b` means Python should multiply the first element in `a` by the first element in `b` and return this value into the first element of the new vector. It will then multiply the second element in `a` by the second element in `b`, and so on. If you type `a ** 2`, then each element in `a` is squared, rather than the entire matrix being matrix-multiplied against itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: LOAD DATA, CONSTRUCT THE GRID, COMPUTE DISTANCES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bandwidth\n",
    "h = 2\n",
    "# TODO: CALCULATE YOUR lambdas HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.**\n",
    "\n",
    "**2.2.1.**\n",
    "\n",
    "Once you have computed the infection rate at a lattice point, plot it in the bubble plot map. If a lattice point has a non-zero infection rate, plot a red dot. We want to make the dot larger where there are more infections, so we tell Python to set the marker size property to be proportional to `lambas`. Adjust the aesthetics of the graph to make it readable. Hint: use `plt.scatter()` and take advantage of the `s` (size) parameter.\n",
    "\n",
    "**2.2.2.**\n",
    "Overlay the locations of the actual infection events using a different marker (that doesn't change size). Do you think this approach identifies the spatial distribution of infection risk well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLOT YOUR MAP HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: ANSWER THE QUESTION HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.**\n",
    "\n",
    "**Rerun your code, but change the bandwidth to `h=1`. Rerun it again but use `h=3`. Turn in both of these maps. How do your three estimates for infection rates compare? Why? What is the advantage or disadvantage of using a larger bandwidth? Do the different bandwidths identify the same infection hotspots?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLOT YOUR MAP FOR h=1 HERE\n",
    "h = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLOT YOUR MAP FOR h=3 HERE\n",
    "h = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: ANSWER THE QUESTION HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Let's Go Fishing: Simulating Poisson Clustering\n",
    "Here you will generate synthetic data that will simulate a Poisson point process (PPP). You'll then compute the K-function for several of your simulations to evaluate the clustering of data in a Poisson point process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.**\n",
    "\n",
    "Consider the space $0<x<10$ and $0<y<10$. This space has an area of 100. Suppose we have a homogenous PPP that generates 120 events on average over this area. What is $\\lambda$? What does this number mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: Answer the question here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2. Let's simulate this PPP.**\n",
    "\n",
    "**3.2.1.**\n",
    "\n",
    "Since the total number of events $z$ must follow a Poisson distribution, draw a random number from this distribution. Since the expected number of events is 120, draw the random variable $z$ using the function `np.random.poisson(lam=120)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.2.**\n",
    "\n",
    "The location of these z events are completely random over the area we are considering, so we can draw their x and y locations from a uniform distribution that goes from zero to ten. Since x and y are uncorrelated, draw these random variables (actually vectors) separately with the function `np.random.random`. We now have the locations of observations for a single simulated realization of our PPP. Plot the observations from this realization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.3.**\n",
    "\n",
    "Repeat this process 3 times. Each time you will draw a new value for z, x and y. How do they compare to one another? Are you surprised?\n",
    "\n",
    "Hint: to repeat this process 3 times, you will want to write a for loop with `range(3)`, some would write `for i in range(3):` but we don't actually need to store `i` because we don't need to keep track of the number of simulations already run. Python has a very nice feature called throwaway variable `_`, and you could write `for _ in range(3):` so that the `_` variable is thrown away. This is not only memory efficient but also syntactically clear. The readers of these codes will know \"oh, you are not planning to do anything with this variable\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WRITE YOUR CODES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: ANSWER THE QUESTION HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.**\n",
    "\n",
    "Write a script to compute the K-function for one of these realizations (or a new one). The procedure you will use is closely related to the calculations you did in the cholera example above, although there are some important differences.\n",
    "\n",
    "* One difference is that you will be computing the distance from each observation to all other observations, rather than the distance from a lattice point to all other observations. The formula is as below\n",
    "$$\\hat{K}(h) = \\frac{1}{z\\lambda}(\\sum_{i=1}^z \\sum_{j \\neq i} \\mathbf{1}\\{D(\\overrightarrow{\\underset{\\cdot}{s_i}}, \\overrightarrow{\\underset{\\cdot}{s_j}}) < h\\})$$\n",
    "* Another difference is that you will average your results over all points (why there is a double summation instead of just one) and output a graph of $K(h)$. Your final results is not a map but a function $K$ defined over distances $h$.\n",
    "* For more on computing the K-function, see Lloyd 7.4.2.\n",
    "\n",
    "Compute $K(h)$ for the integer values of $h=\\{1,2,3...,7\\}$. You don't need to try and compute every value in between. You can plot the K-function as a line using `plt.plot(h, K, '-')` (this interpolates for you). Overlay on this figure a plot of ${\\pi \\times h^2}$ using a different color (recall that this is the expected K-function for a PPP). How do they compare and why?\n",
    "\n",
    "Notes: What we've done in the above blocks is copy-pasting lots of codes. Copy-pasting code is in general a bad practice and is strongly discouraged: (1) What if you want to change something about your codes (e.g., you realized later on that there is a bug somewhere)? You would need to fix it multiple times. (2) What if someone else (most likely your future self) wants to read your codes and understand what you were doing? They would then have to read the same block of codes over and over again (and wonder about whether there are small changes made to each copy).\n",
    "\n",
    "What is encouraged, then, is to write functions. Functions keep your codes organized, easy to maintain, and recycleable. In earlier labs we will be nudging you to write functions by writing the docstrings for you, and in later labs we expect that you will be writing functions on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(h, new=True, z=None, x=None, y=None):\n",
    "    \"\"\"Calculates the K function for different bandwidths.\n",
    "    \n",
    "    Args:\n",
    "        h (numpy.ndarray): an array of bandwidths.\n",
    "        new (bool): whether a new simulation should be carried out.\n",
    "            if False, x, y, z arguments need to be supplied.\n",
    "        x, y (numpy.ndarray): arrays containing x, y coordinates of the events.\n",
    "        z (int): number of events in a realization of the simulation\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: a array of K(h), of the same length as h.\n",
    "    \"\"\"\n",
    "    # these bool inputs (flags) control the behavior of the function\n",
    "    # allowing you to write functions that, for example,\n",
    "    # take different kinds of input\n",
    "    if new:\n",
    "        # TODO: WRITE CODES HERE TO COMPLETE THE SECTION\n",
    "        pass\n",
    "    # TODO: COMPLETE THIS FUNCTION\n",
    "\n",
    "\n",
    "# TODO: USING THE FUNCTION, ANSWER THE QUESTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: ANSWER THE QUESTION HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.**\n",
    "\n",
    "**3.4.1.**\n",
    "\n",
    "Now expand your script to simulate the PPP fifty times and calculate the K-function each time. Don't bother storing the $x$ and $y$ values to each simulation, just store the values of $K(h)$. Plot all 50 estimates for $K(h)$ on a single plot using a single color. Again, overlay on this figure a plot of $\\pi \\times h^2$ using a different color. **How do they compare and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.2.**\n",
    "\n",
    "The dataset `Lab3_poisson_test_data.p` contains location data from a different point process. We want to test if it is more or less clustered than a PPP. Compute the K-function for this dataset at the same values for $h$. Plot this K-function overlaid with your earlier plot of 50 K-functions from the simulated PPP (using a third color). **Does the testing data look more, less or similarly clustered to the PPP data?** Plot a map of the testing data to check whether your interpretation makes intuitive sense or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WRITE YOUR CODES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLOT TEST DATA HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: ANSWER THE QUESTIONS HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "You've finished this lab!\n",
    "\n",
    "* **Please list everyone you worked on this assignment with outside of public Piazza discussions.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
